# Offline RAG System (Ollama + FAISS)

A **fully offline Retrieval-Augmented Generation (RAG) system** built in Python.
This project allows you to ingest documents, generate and persist embeddings, and query them locally using an LLM â€” **without any internet connection once models are installed**.

The system is optimized for performance by **persisting document embeddings** and **caching queries and responses in memory**.

---

## âœ¨ Features

- ğŸ”’ **100% Offline RAG** (after model setup)
- ğŸ“„ Document ingestion and chunking
- ğŸ§  Embedding generation using **nomic-embed-text**
- ğŸ—„ï¸ Vector search powered by **FAISS**
- ğŸ’¾ Persistent embeddings stored as `.npy` files (no re-embedding on restart)
- âš¡ In-memory caching for:

  - Query embeddings
  - Model responses

- ğŸ–¥ï¸ Simple **command-line interface (CLI)**
- ğŸ§© Modular and easy to extend

---

## ğŸ§± Tech Stack

- **LLM Runtime:** Ollama
- **LLM:** llama3.2
- **Embedding Model:** nomic-embed-text
- **Vector Database:** FAISS
- **Language:** Python (3.8+)

---

## ğŸ—ï¸ Architecture Overview

1. Documents are ingested from disk
2. Documents are chunked into smaller text segments
3. Chunks are embedded using `nomic-embed-text`
4. Embeddings are:

   - Indexed in FAISS
   - Persisted to disk as `.npy` files

5. User queries (via CLI) are:

   - Embedded
   - Cached in memory

6. Relevant chunks are retrieved via FAISS similarity search
7. Retrieved context is passed to `llama3.2`
8. Final responses are:

   - Returned to the user
   - Cached in memory for faster repeat queries

> âš ï¸ Note: Only **document embeddings** are persisted. Query and response caches are **in-memory only** for now.

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Ollama

Download and install Ollama from:

ğŸ‘‰ [https://ollama.com](https://ollama.com)

---

### 2ï¸âƒ£ Install Required Models

Once Ollama is installed, pull the required models:

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

These models are stored locally and used fully offline.

---

### 3ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/miracletim/faiss-rag-offline.git
cd faiss-rag-offline
```

---

### 4ï¸âƒ£ Install Python Dependencies

Ensure you have **Python 3.8 or higher**, then run:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running the System

Simply run the app entry point:

```bash
python app.py
```

The system is **self-guided** and will:

- Inform you if required folders, files, or models are missing
- Guide you through the setup if something is not configured correctly

---

## ğŸ“‚ Embedding Persistence

- Document embeddings are saved as `.npy` files
- On subsequent runs, embeddings are **loaded from disk** instead of recomputed
- This significantly improves startup and query performance

---

## ğŸ§  Caching Strategy

| Cached Item         | Storage       | Persisted |
| ------------------- | ------------- | --------- |
| Document embeddings | Disk (`.npy`) | âœ… Yes    |
| Query embeddings    | Memory        | âŒ No     |
| LLM responses       | Memory        | âŒ No     |

---

## ğŸ“Œ Requirements

- Python **3.8+**
- Ollama (installed locally)
- llama3.2 model
- nomic-embed-text model

All Python dependencies are listed in `requirements.txt`.

---

## ğŸ”® Future Improvements

- Persist query & response cache
- Support for multiple embedding files
- Configurable chunk sizes
- Streaming responses
- Optional UI (web or desktop)

---

## ğŸ¤ Contributing

Contributions, ideas, and improvements are welcome. Feel free to fork the repo and submit a pull request.

---

## ğŸ“œ License

MIT

---

## ğŸ§  Author

**Miracle Timothy**
Full Stack Developer | AI Systems Builder

---

> _"Offline-first AI systems are not a limitation â€” they are a design choice."_ ğŸš€
